{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5b6593b-1d46-4638-8439-af2d7aecd2de",
   "metadata": {},
   "source": [
    "# SageMaker Model Monitoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e2d27",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a14d801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Education                     int8\n",
      "JoiningYear                  int64\n",
      "City                          int8\n",
      "PaymentTier                  int64\n",
      "Age                          int64\n",
      "Gender                        int8\n",
      "EverBenched                  int64\n",
      "ExperienceInCurrentDomain    int64\n",
      "LeaveOrNot                   int64\n",
      "dtype: object\n",
      "Training data uploaded to s3://sagemaker-ml-28573/model-monitor/train/train.csv\n",
      "Validation data uploaded to s3://sagemaker-ml-28573/model-monitor/validation/validation.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# S3 bucket for storing data\n",
    "bucket = 'sagemaker-ml-28573'\n",
    "prefix = 'model-monitor'\n",
    "output_path = f's3://{bucket}/{prefix}/output'\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Employee.csv'  # Replace with your actual file path in S3 if needed\n",
    "employee_df = pd.read_csv(file_path)\n",
    "employee_df.head()\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Data Preparation\n",
    "# Convert categorical columns to numeric\n",
    "employee_df['Education'] = employee_df['Education'].astype('category').cat.codes\n",
    "employee_df['City'] = employee_df['City'].astype('category').cat.codes\n",
    "employee_df['Gender'] = employee_df['Gender'].astype('category').cat.codes\n",
    "employee_df['EverBenched'] = employee_df['EverBenched'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Drop rows with NaN values in the target column\n",
    "employee_df.dropna(subset=['LeaveOrNot'])\n",
    "\n",
    "# Convert target column to numeric if needed\n",
    "employee_df['LeaveOrNot'] = employee_df['LeaveOrNot'].astype(int)\n",
    "\n",
    "# Ensure no missing values in feature columns\n",
    "employee_df = employee_df.dropna()\n",
    "\n",
    "# Verify all columns are numeric\n",
    "print(employee_df.dtypes)\n",
    "\n",
    "# Define features and target\n",
    "feature_columns = [\n",
    "    'Education', 'JoiningYear', 'City', 'PaymentTier', 'Age',\n",
    "    'Gender', 'EverBenched', 'ExperienceInCurrentDomain'\n",
    "]\n",
    "target_column = 'LeaveOrNot'\n",
    "\n",
    "employee_df = employee_df[[target_column] + feature_columns]\n",
    "\n",
    "train_df, test_df = train_test_split(employee_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the transformed dataset\n",
    "employee_df.head()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "\n",
    "# Save the data locally first\n",
    "train_file = 'train.csv'\n",
    "validation_file = 'validation.csv'\n",
    "train_df.to_csv(train_file, index=False)\n",
    "test_df.to_csv(validation_file, index=False)\n",
    "\n",
    "# Upload the data to S3\n",
    "s3.upload_file(train_file, bucket, f'{prefix}/train/{train_file}')\n",
    "s3.upload_file(validation_file, bucket, f'{prefix}/validation/{validation_file}')\n",
    "\n",
    "print(f\"Training data uploaded to s3://{bucket}/{prefix}/train/{train_file}\")\n",
    "print(f\"Validation data uploaded to s3://{bucket}/{prefix}/validation/{validation_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca50e267-39f2-45f3-bc39-b4065b6bf0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-09-03-12-15-08-548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to s3://sagemaker-ml-28573/deployment/train/train.csv\n",
      "Validation data uploaded to s3://sagemaker-ml-28573/deployment/validation/validation.csv\n",
      "2024-09-03 12:15:10 Starting - Starting the training job...\n",
      "2024-09-03 12:15:23 Starting - Preparing the instances for training...\n",
      "2024-09-03 12:16:11 Downloading - Downloading the training image......\n",
      "2024-09-03 12:17:02 Training - Training image download completed. Training in progress.\n",
      "2024-09-03 12:17:02 Uploading - Uploading generated training model\u001b[34m[2024-09-03 12:16:57.415 ip-10-2-209-70.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-09-03 12:16:57.437 ip-10-2-209-70.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-09-03:12:16:57:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2024-09-03:12:16:57:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2024-09-03:12:16:57:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-09-03:12:16:57:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2024-09-03:12:16:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-09-03:12:16:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-09-03:12:16:57:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2024-09-03:12:16:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-09-03:12:16:57:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[2024-09-03:12:16:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-09-03:12:16:57:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2024-09-03:12:16:57:INFO] Train matrix has 3723 rows and 8 columns\u001b[0m\n",
      "\u001b[34m[2024-09-03:12:16:57:INFO] Validation matrix has 932 rows\u001b[0m\n",
      "\u001b[34m[2024-09-03 12:16:57.500 ip-10-2-209-70.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-09-03 12:16:57.501 ip-10-2-209-70.ec2.internal:7 INFO hook.py:207] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-09-03 12:16:57.502 ip-10-2-209-70.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-09-03 12:16:57.502 ip-10-2-209-70.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-09-03:12:16:57:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[12:16:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\u001b[0m\n",
      "\u001b[34m[2024-09-03 12:16:57.508 ip-10-2-209-70.ec2.internal:7 INFO hook.py:428] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2024-09-03 12:16:57.510 ip-10-2-209-70.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[0]#011train-logloss:0.61438#011validation-logloss:0.60751\u001b[0m\n",
      "\u001b[34m[1]#011train-logloss:0.55715#011validation-logloss:0.54619\u001b[0m\n",
      "\u001b[34m[2]#011train-logloss:0.51811#011validation-logloss:0.50401\u001b[0m\n",
      "\u001b[34m[3]#011train-logloss:0.49991#011validation-logloss:0.48395\u001b[0m\n",
      "\u001b[34m[4]#011train-logloss:0.48661#011validation-logloss:0.46905\u001b[0m\n",
      "\u001b[34m[5]#011train-logloss:0.46939#011validation-logloss:0.45047\u001b[0m\n",
      "\u001b[34m[6]#011train-logloss:0.46506#011validation-logloss:0.44522\u001b[0m\n",
      "\u001b[34m[7]#011train-logloss:0.45596#011validation-logloss:0.43293\u001b[0m\n",
      "\u001b[34m[8]#011train-logloss:0.45035#011validation-logloss:0.42719\u001b[0m\n",
      "\u001b[34m[9]#011train-logloss:0.44423#011validation-logloss:0.42068\u001b[0m\n",
      "\u001b[34m[10]#011train-logloss:0.44393#011validation-logloss:0.42025\u001b[0m\n",
      "\u001b[34m[11]#011train-logloss:0.44375#011validation-logloss:0.41998\u001b[0m\n",
      "\u001b[34m[12]#011train-logloss:0.44363#011validation-logloss:0.41976\u001b[0m\n",
      "\u001b[34m[13]#011train-logloss:0.44361#011validation-logloss:0.41970\u001b[0m\n",
      "\u001b[34m[14]#011train-logloss:0.44356#011validation-logloss:0.41959\u001b[0m\n",
      "\u001b[34m[15]#011train-logloss:0.44355#011validation-logloss:0.41955\u001b[0m\n",
      "\u001b[34m[16]#011train-logloss:0.44355#011validation-logloss:0.41956\u001b[0m\n",
      "\u001b[34m[17]#011train-logloss:0.43763#011validation-logloss:0.41146\u001b[0m\n",
      "\u001b[34m[18]#011train-logloss:0.43764#011validation-logloss:0.41148\u001b[0m\n",
      "\u001b[34m[19]#011train-logloss:0.43763#011validation-logloss:0.41146\u001b[0m\n",
      "\u001b[34m[20]#011train-logloss:0.43763#011validation-logloss:0.41145\u001b[0m\n",
      "\u001b[34m[21]#011train-logloss:0.43763#011validation-logloss:0.41146\u001b[0m\n",
      "\u001b[34m[22]#011train-logloss:0.43763#011validation-logloss:0.41146\u001b[0m\n",
      "\u001b[34m[23]#011train-logloss:0.43764#011validation-logloss:0.41148\u001b[0m\n",
      "\u001b[34m[24]#011train-logloss:0.43765#011validation-logloss:0.41151\u001b[0m\n",
      "\u001b[34m[25]#011train-logloss:0.43765#011validation-logloss:0.41151\u001b[0m\n",
      "\u001b[34m[26]#011train-logloss:0.43764#011validation-logloss:0.41148\u001b[0m\n",
      "\u001b[34m[27]#011train-logloss:0.43764#011validation-logloss:0.41149\u001b[0m\n",
      "\u001b[34m[28]#011train-logloss:0.43763#011validation-logloss:0.41145\u001b[0m\n",
      "\u001b[34m[29]#011train-logloss:0.43763#011validation-logloss:0.41144\u001b[0m\n",
      "\u001b[34m[30]#011train-logloss:0.43765#011validation-logloss:0.41151\u001b[0m\n",
      "\u001b[34m[31]#011train-logloss:0.43765#011validation-logloss:0.41152\u001b[0m\n",
      "\u001b[34m[32]#011train-logloss:0.43765#011validation-logloss:0.41153\u001b[0m\n",
      "\u001b[34m[33]#011train-logloss:0.43765#011validation-logloss:0.41152\u001b[0m\n",
      "\u001b[34m[34]#011train-logloss:0.43764#011validation-logloss:0.41151\u001b[0m\n",
      "\u001b[34m[35]#011train-logloss:0.43764#011validation-logloss:0.41149\u001b[0m\n",
      "\u001b[34m[36]#011train-logloss:0.43763#011validation-logloss:0.41145\u001b[0m\n",
      "\u001b[34m[37]#011train-logloss:0.43763#011validation-logloss:0.41145\u001b[0m\n",
      "\u001b[34m[38]#011train-logloss:0.43763#011validation-logloss:0.41143\u001b[0m\n",
      "\u001b[34m[39]#011train-logloss:0.43763#011validation-logloss:0.41141\u001b[0m\n",
      "\u001b[34m[40]#011train-logloss:0.43763#011validation-logloss:0.41144\u001b[0m\n",
      "\u001b[34m[41]#011train-logloss:0.43478#011validation-logloss:0.40798\u001b[0m\n",
      "\u001b[34m[42]#011train-logloss:0.43479#011validation-logloss:0.40803\u001b[0m\n",
      "\u001b[34m[43]#011train-logloss:0.43480#011validation-logloss:0.40804\u001b[0m\n",
      "\u001b[34m[44]#011train-logloss:0.43478#011validation-logloss:0.40800\u001b[0m\n",
      "\u001b[34m[45]#011train-logloss:0.43478#011validation-logloss:0.40799\u001b[0m\n",
      "\u001b[34m[46]#011train-logloss:0.43479#011validation-logloss:0.40801\u001b[0m\n",
      "\u001b[34m[47]#011train-logloss:0.43478#011validation-logloss:0.40799\u001b[0m\n",
      "\u001b[34m[48]#011train-logloss:0.43478#011validation-logloss:0.40799\u001b[0m\n",
      "\u001b[34m[49]#011train-logloss:0.43478#011validation-logloss:0.40797\u001b[0m\n",
      "\n",
      "2024-09-03 12:17:15 Completed - Training job completed\n",
      "Training seconds: 84\n",
      "Billable seconds: 84\n"
     ]
    }
   ],
   "source": [
    "### CAN BE SKIPPED ####\n",
    "\n",
    "# Setup XGBoost Estimator\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"1.3-1\")\n",
    "hyperparameters = {\n",
    "    \"max_depth\":\"5\",\n",
    "    \"eta\":\"0.2\",\n",
    "    \"gamma\":\"40\",\n",
    "    \"min_child_weight\":\"6\",\n",
    "    \"subsample\":\"0.7\",\n",
    "    \"objective\":\"binary:logistic\",\n",
    "    \"num_round\":\"50\"\n",
    "}\n",
    "\n",
    "output_path = f's3://{bucket}/{prefix}/output'\n",
    "\n",
    "estimator = Estimator(\n",
    "    image_uri=xgboost_container, \n",
    "    hyperparameters=hyperparameters,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.xlarge', \n",
    "    volume_size=5,  # 5 GB \n",
    "    output_path=output_path\n",
    ")\n",
    "\n",
    "# Define the data type and paths to the training and validation datasets\n",
    "content_type = \"csv\"\n",
    "train_input = TrainingInput(f\"s3://{bucket}/{prefix}/train/{train_file}\", content_type=content_type)\n",
    "validation_input = TrainingInput(f\"s3://{bucket}/{prefix}/validation/{validation_file}\", content_type=content_type)\n",
    "\n",
    "# Execute the XGBoost training job\n",
    "estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921e2420",
   "metadata": {},
   "source": [
    "## Deploy as Real-Time Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ac3e31ae-4698-4c97-9904-486659a6e2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2024-09-04-13-27-32-546\n",
      "INFO:sagemaker:Creating endpoint-config with name employee-monitor-endpoint\n",
      "INFO:sagemaker:Creating endpoint with name employee-monitor-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "output_path = f's3://{bucket}/{prefix}/output'\n",
    "s3_capture_upload_path = output_path\n",
    "# Specify the S3 path to the pre-trained model artifact\n",
    "model_artifact = \"s3://sagemaker-ml-28573/deployment/output/sagemaker-xgboost-2024-09-03-12-15-08-548/output/model.tar.gz\"\n",
    "\n",
    "# Specify the endpoint name\n",
    "endpoint_name = 'employee-monitor-endpoint'\n",
    "\n",
    "# Retrieve the container image for the framework (e.g., XGBoost)\n",
    "container = sagemaker.image_uris.retrieve(framework=\"xgboost\", region=boto3.Session().region_name, version=\"1.3-1\")\n",
    "\n",
    "# Create the model object using the S3 path\n",
    "model = Model(\n",
    "    image_uri=container,\n",
    "    model_data=model_artifact,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True, sampling_percentage=100, destination_s3_uri=s3_capture_upload_path\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    data_capture_config=data_capture_config,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a8dbf3fb-861d-4d5c-a1d4-40d89a5c270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20303820073604584\n",
      "0.20303820073604584\n",
      "0.3774380087852478\n",
      "0.5340070128440857\n",
      "0.1691509336233139\n",
      "0.37122291326522827\n",
      "0.6700358986854553\n",
      "0.8091390132904053\n",
      "0.1691509336233139\n",
      "0.8091390132904053\n",
      "0.1691509336233139\n",
      "0.20303820073604584\n",
      "0.9251188635826111\n",
      "0.1691509336233139\n",
      "0.35528063774108887\n",
      "0.8091390132904053\n",
      "0.1633274257183075\n",
      "0.38120347261428833\n",
      "0.36576730012893677\n",
      "0.1691509336233139\n",
      "0.1691509336233139\n",
      "0.40060654282569885\n",
      "0.7384953498840332\n",
      "0.20303820073604584\n",
      "0.3876122534275055\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the SageMaker runtime client\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "\n",
    "\n",
    "# Prepare your input data (same format as before)\n",
    "test_data = test_df[feature_columns].head(25)  # Select the first row of test data for prediction\n",
    "test_data_csv = test_data.to_csv(index=False, header=False).strip()  # Convert to CSV format\n",
    "\n",
    "# Invoke the endpoint directly using the runtime client\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"text/csv\",  # Specify the content type\n",
    "    Body=test_data_csv  # The input data as a CSV string\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "result = response['Body'].read().decode('utf-8')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23183684-5ca9-4c7c-b0b5-14c2afa65520",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Model Monitor: Setting Up a Baseline Using Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a1cf1694-6db2-4cda-93d6-92126cdbbd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline data uri: s3://sagemaker-ml-28573/model-monitor/train/train.csv\n",
      "Baseline results uri: s3://sagemaker-ml-28573/model-monitor/baselining/results\n"
     ]
    }
   ],
   "source": [
    "# set up paths\n",
    "prefix = 'model-monitor'\n",
    "baseline_prefix = f\"s3://{bucket}/{prefix}/baselining\" \n",
    "baseline_data = f\"s3://{bucket}/{prefix}/train/{train_file}\" \n",
    "baseline_results = f\"{baseline_prefix}/results\"   \n",
    "\n",
    "\n",
    "print(\"Baseline data uri: {}\".format(baseline_data))\n",
    "print(\"Baseline results uri: {}\".format(baseline_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "321e40b9-29f4-486f-89c6-5d6832e4d224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2024-09-04-13-31-05-215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................\u001b[34m2024-09-04 13:33:51.577716: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:51.577755: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:53.639934: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:53.639972: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:53.640002: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-99-143.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:53.640334: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:55,528 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:448049810900:processing-job/baseline-suggestion-job-2024-09-04-13-31-05-215', 'ProcessingJobName': 'baseline-suggestion-job-2024-09-04-13-31-05-215', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-ml-28573/model-monitor/train/train.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-ml-28573/model-monitor/baselining/results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.t3.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::448049810900:role/service-role/AmazonSageMaker-ExecutionRole-20240823T111826', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:55,528 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:55,529 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:55,529 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:55,529 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:55,529 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:55,580 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:55,581 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:55,581 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.t3.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.t3.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0'}\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:55,586 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:55,586 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:55,586 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:56,233 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.99.143\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-\u001b[0m\n",
      "\u001b[34mnodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:56,245 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:56,250 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-3d8a9bff-9b72-4174-9ede-b1b02fa26a4f\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,076 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,093 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,094 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,096 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,102 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,102 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,103 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,103 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,142 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,155 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,155 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,160 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,164 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Sep 04 13:33:57\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,167 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,167 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,168 INFO util.GSet: 2.0% max memory 3.1 GB = 63.1 MB\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,168 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,208 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,213 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,213 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,213 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,213 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,214 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,214 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,214 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,214 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,214 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,214 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,214 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,246 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,246 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,246 INFO util.GSet: 1.0% max memory 3.1 GB = 31.6 MB\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,246 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,248 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,248 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,248 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,249 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,258 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,263 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,263 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,264 INFO util.GSet: 0.25% max memory 3.1 GB = 7.9 MB\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,264 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,305 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,305 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,305 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,310 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,310 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,312 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,312 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,312 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 970.0 KB\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,312 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,348 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1601288927-10.0.99.143-1725456837342\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,359 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,368 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,462 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,475 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,479 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.99.143\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:57,493 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:59,565 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2024-09-04 13:33:59,566 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:01,708 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:01,709 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:03,954 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:03,954 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:06,253 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:06,253 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:08,632 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:08,633 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:18,644 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:20,963 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:21,561 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:21,625 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:21,639 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:22,449 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:22,495 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:22,496 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:22,496 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:22,498 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:22,547 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11588, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:22,582 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:22,586 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:22,691 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:22,692 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:22,693 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:22,696 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:22,697 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:23,246 INFO util.Utils: Successfully started service 'sparkDriver' on port 37565.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:23,358 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:23,413 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:23,444 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:23,444 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:23,496 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:23,525 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-d39f837f-aa9f-421e-852c-315c27dbfe84\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:23,553 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:23,636 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:23,685 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.99.143:37565/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1725456862441\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:24,442 INFO client.RMProxy: Connecting to ResourceManager at /10.0.99.143:8032\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:25,684 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:25,685 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:25,694 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15802 MB per container)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:25,695 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:25,695 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:25,696 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:25,704 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:25,836 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:28,647 INFO yarn.Client: Uploading resource file:/tmp/spark-703972d2-4a6d-4efa-9243-c5eb30d35c14/__spark_libs__8064551770871140381.zip -> hdfs://10.0.99.143/user/root/.sparkStaging/application_1725456844564_0001/__spark_libs__8064551770871140381.zip\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:30,068 INFO yarn.Client: Uploading resource file:/tmp/spark-703972d2-4a6d-4efa-9243-c5eb30d35c14/__spark_conf__7381664073205598824.zip -> hdfs://10.0.99.143/user/root/.sparkStaging/application_1725456844564_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:30,536 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:30,536 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:30,537 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:30,537 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:30,538 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:30,575 INFO yarn.Client: Submitting application application_1725456844564_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:30,901 INFO impl.YarnClientImpl: Submitted application application_1725456844564_0001\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:31,908 INFO yarn.Client: Application report for application_1725456844564_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:31,916 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Wed Sep 04 13:34:31 +0000 2024] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1725456870707\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1725456844564_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:32,920 INFO yarn.Client: Application report for application_1725456844564_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:33,927 INFO yarn.Client: Application report for application_1725456844564_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:34,932 INFO yarn.Client: Application report for application_1725456844564_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:35,937 INFO yarn.Client: Application report for application_1725456844564_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:36,945 INFO yarn.Client: Application report for application_1725456844564_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:37,950 INFO yarn.Client: Application report for application_1725456844564_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:38,654 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1725456844564_0001), /proxy/application_1725456844564_0001\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:38,954 INFO yarn.Client: Application report for application_1725456844564_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:38,954 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.99.143\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1725456870707\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1725456844564_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:38,956 INFO cluster.YarnClientSchedulerBackend: Application application_1725456844564_0001 has started running.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:38,966 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35807.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:38,966 INFO netty.NettyBlockTransferService: Server created on 10.0.99.143:35807\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:38,968 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:38,982 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.99.143, 35807, None)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:38,987 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.99.143:35807 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.99.143, 35807, None)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:38,992 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.99.143, 35807, None)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:38,994 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.99.143, 35807, None)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:39,144 INFO util.log: Logging initialized @20111ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:40,542 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:45,140 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.99.143:57506) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:45,400 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:45765 with 5.9 GiB RAM, BlockManagerId(1, algo-1, 45765, None)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:54,230 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:54,630 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:54,719 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:54,725 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:56,371 INFO datasources.InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:56,655 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:57,205 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:57,209 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.99.143:35807 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:57,226 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:57,736 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:57,740 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:57,745 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 81983\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:57,835 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:57,856 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:57,856 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:57,857 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:57,858 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:57,869 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:57,988 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:57,992 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:57,994 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.99.143:35807 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:57,995 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:58,026 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:58,028 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:58,106 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4618 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:58,596 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:45765 (size: 4.2 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:34:59,632 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:45765 (size: 39.2 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:00,203 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2118 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:00,217 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:00,232 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 2.283 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:00,238 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:00,238 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:00,241 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 2.406130 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:00,501 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.99.143:35807 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:00,502 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:45765 in memory (size: 4.2 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:00,588 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:45765 in memory (size: 39.2 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:00,596 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.99.143:35807 in memory (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:05,418 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:05,423 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:05,429 INFO datasources.FileSourceStrategy: Output Data Schema: struct<LeaveOrNot: string, Education: string, JoiningYear: string, City: string, PaymentTier: string ... 7 more fields>\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:05,812 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:05,831 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:05,832 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.99.143:35807 (size: 39.1 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:05,834 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:05,872 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:05,971 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:05,974 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:05,974 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:05,974 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:05,980 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:05,990 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:06,104 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.4 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:06,112 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:06,113 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.99.143:35807 (size: 7.9 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:06,113 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:06,114 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:06,114 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:06,123 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:06,204 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:45765 (size: 7.9 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:07,661 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:45765 (size: 39.1 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:08,172 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:45765 (size: 66.3 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:08,385 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2268 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:08,386 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:08,389 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 2.388 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:08,390 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:08,390 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:08,391 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 2.419941 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:08,752 INFO codegen.CodeGenerator: Code generated in 260.150189 ms\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:09,464 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:45765 in memory (size: 7.9 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:09,483 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.99.143:35807 in memory (size: 7.9 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:09,804 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:10,003 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:10,008 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:10,008 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:10,009 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:10,012 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:10,014 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:10,042 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 113.5 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:10,044 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:10,045 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.99.143:35807 (size: 34.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:10,046 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:10,049 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:10,049 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:10,059 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:10,088 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:45765 (size: 34.5 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,084 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2027 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,084 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,087 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 2.067 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,090 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,091 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,092 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,092 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,367 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,378 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,378 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,378 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,378 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,396 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,447 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 166.2 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,456 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 45.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,460 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.99.143:35807 (size: 45.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,465 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,467 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,467 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,479 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,564 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:45765 (size: 45.9 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:12,643 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.99.143:57506\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,300 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 826 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,301 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,302 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.883 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,303 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,303 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,303 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.936112 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,352 INFO codegen.CodeGenerator: Code generated in 37.659009 ms\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,756 INFO codegen.CodeGenerator: Code generated in 39.224222 ms\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,867 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,869 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,869 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,869 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,870 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,871 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,903 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 36.8 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,906 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,908 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.99.143:35807 (size: 16.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,909 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,910 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,910 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,914 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:13,947 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:45765 (size: 16.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:14,468 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 555 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:14,470 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.597 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:14,471 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:14,471 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:14,471 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:14,472 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.604835 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,460 INFO codegen.CodeGenerator: Code generated in 201.139175 ms\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,480 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,482 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,482 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,482 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,484 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,486 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,508 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 74.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,519 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,521 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.99.143:35807 (size: 24.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,524 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,526 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,528 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,531 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,553 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:45765 (size: 24.2 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,723 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 193 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,723 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,725 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.234 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,725 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,725 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,725 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:15,725 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,091 INFO codegen.CodeGenerator: Code generated in 192.623822 ms\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,118 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,120 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,120 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,120 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,121 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,122 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,131 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 67.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,133 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,134 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.99.143:35807 (size: 19.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,134 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,135 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,135 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,138 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,164 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:45765 (size: 19.8 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,173 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.99.143:57506\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,256 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 117 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,256 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,257 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.130 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,258 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,259 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,260 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.141862 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,500 INFO codegen.CodeGenerator: Code generated in 196.908067 ms\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,670 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,675 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,676 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,676 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,676 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,677 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,679 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,690 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 29.6 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,692 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 13.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,693 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.99.143:35807 (size: 13.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,694 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,694 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,694 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,696 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:16,710 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:45765 (size: 13.7 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,691 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1996 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,691 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,692 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 2.011 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,692 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,692 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,692 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,692 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,693 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,697 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,699 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,700 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.99.143:35807 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,701 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,701 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,701 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,708 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,727 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:45765 (size: 3.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,735 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.99.143:57506\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,796 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 91 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,797 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,803 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.109 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,804 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,804 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:18,805 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 2.134858 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,069 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,069 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,069 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,069 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,070 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,071 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,076 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 72.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,078 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 25.6 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,079 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.99.143:35807 (size: 25.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,080 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,080 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,080 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,082 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,098 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:45765 (size: 25.6 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,594 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 512 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,595 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,597 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.524 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,597 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,598 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,599 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,599 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,667 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,669 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,670 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,670 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,670 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,671 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,680 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 140.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,683 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 40.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,684 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.99.143:35807 (size: 40.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,685 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,686 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,686 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,692 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,707 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:45765 (size: 40.1 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,718 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.99.143:57506\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,848 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 156 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,848 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,851 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.177 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,852 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,853 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,854 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.186001 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:19,874 INFO codegen.CodeGenerator: Code generated in 16.301854 ms\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,181 INFO codegen.CodeGenerator: Code generated in 63.966769 ms\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,300 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,302 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,302 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,302 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,304 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,306 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,321 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 35.8 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,323 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,329 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.99.143:35807 (size: 15.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,330 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,331 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,335 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,338 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,351 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:45765 (size: 15.9 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,566 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 228 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,567 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,568 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.258 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,568 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,568 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,568 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.267758 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,664 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:45765 in memory (size: 45.9 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,668 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.99.143:35807 in memory (size: 45.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,706 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:45765 in memory (size: 3.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,717 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.99.143:35807 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,728 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:45765 in memory (size: 40.1 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,751 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.99.143:35807 in memory (size: 40.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,789 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:45765 in memory (size: 16.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,795 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.99.143:35807 in memory (size: 16.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,810 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.99.143:35807 in memory (size: 19.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,812 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:45765 in memory (size: 19.8 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,875 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.99.143:35807 in memory (size: 34.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,894 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:45765 in memory (size: 34.5 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,941 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.99.143:35807 in memory (size: 13.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,944 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:45765 in memory (size: 13.7 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,981 INFO codegen.CodeGenerator: Code generated in 101.906954 ms\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,984 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:45765 in memory (size: 24.2 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:20,989 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.99.143:35807 in memory (size: 24.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,001 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,002 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,003 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,003 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,004 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,005 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,026 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 64.2 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,029 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 21.6 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,030 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.99.143:35807 (size: 21.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,030 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,033 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,033 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,038 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,058 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.99.143:35807 in memory (size: 15.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,071 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:45765 in memory (size: 15.9 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,077 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:45765 (size: 21.6 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,183 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.99.143:35807 in memory (size: 25.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,202 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:45765 in memory (size: 25.6 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,314 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 277 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,319 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,320 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.304 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,321 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,321 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,321 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,321 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,523 INFO codegen.CodeGenerator: Code generated in 50.306443 ms\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,540 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,541 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,542 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,542 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,542 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,543 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,548 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 56.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,551 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 17.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,551 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.99.143:35807 (size: 17.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,553 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,553 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,554 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,556 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,587 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:45765 (size: 17.2 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,605 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.99.143:57506\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,740 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 185 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,740 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,742 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.196 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,745 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,747 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,748 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.207949 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,863 INFO codegen.CodeGenerator: Code generated in 85.50789 ms\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,984 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,988 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,991 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,994 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,994 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,995 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:21,996 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,011 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 29.6 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,029 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.7 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,030 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.99.143:35807 (size: 13.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,045 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,046 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,046 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,050 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,080 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:45765 (size: 13.7 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,266 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 217 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,266 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,267 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.270 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,269 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,269 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,269 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,270 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,270 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,275 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,278 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,279 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.99.143:35807 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,301 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,303 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,304 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,307 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,339 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:45765 (size: 3.0 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,351 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.99.143:57506\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,388 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 81 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,390 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.119 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,390 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,391 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,391 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,391 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.406628 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,830 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,889 INFO codegen.CodeGenerator: Code generated in 20.81453 ms\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,905 INFO scheduler.DAGScheduler: Registering RDD 86 (count at StatsGenerator.scala:66) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,905 INFO scheduler.DAGScheduler: Got map stage job 14 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,906 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,906 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,907 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,908 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,916 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 21.6 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,921 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,923 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.99.143:35807 (size: 10.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,924 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,925 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,930 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,932 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:22,951 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:45765 (size: 10.1 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,034 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 102 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,035 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,036 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (count at StatsGenerator.scala:66) finished in 0.125 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,037 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,037 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,037 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,038 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,074 INFO codegen.CodeGenerator: Code generated in 27.098559 ms\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,089 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,090 INFO scheduler.DAGScheduler: Got job 15 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,090 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,091 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,091 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,091 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,094 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 11.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,096 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,098 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.99.143:35807 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,098 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,099 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,099 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,101 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,122 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:45765 (size: 5.5 KiB, free: 5.9 GiB)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,143 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.99.143:57506\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,210 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 109 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,211 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,211 INFO scheduler.DAGScheduler: ResultStage 22 (count at StatsGenerator.scala:66) finished in 0.118 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,212 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,212 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,213 INFO scheduler.DAGScheduler: Job 15 finished: count at StatsGenerator.scala:66, took 0.123601 s\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,525 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,537 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,607 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,608 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,616 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,649 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,745 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,750 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,755 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,763 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,830 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,831 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,831 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,861 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,869 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-703972d2-4a6d-4efa-9243-c5eb30d35c14\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,901 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3bffbdaa-c525-49ce-b16a-3fb68df321b7\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:23,999 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2024-09-04 13:35:24,000 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7fd2228f4ee0>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "\n",
    "my_default_monitor_1 = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.t3.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor_1.suggest_baseline(\n",
    "    baseline_dataset=baseline_data,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad3aff-ad37-4c31-8856-437118cb9645",
   "metadata": {},
   "source": [
    "### Explore generated constraints & statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b5b34d75-45de-477c-91b9-121ceaa298c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Files:\n",
      "s3://sagemaker-ml-28573/model monitor/baselining/results/constraints.json\n",
      " s3://sagemaker-ml-28573/model monitor/baselining/results/statistics.json\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84d0146c-51f9-48f4-8306-d837ccadab34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2bf0ac77-da9d-4f82-8de7-a1a1373e0d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>numerical_statistics.common.num_present</th>\n",
       "      <th>numerical_statistics.common.num_missing</th>\n",
       "      <th>numerical_statistics.mean</th>\n",
       "      <th>numerical_statistics.sum</th>\n",
       "      <th>numerical_statistics.std_dev</th>\n",
       "      <th>numerical_statistics.min</th>\n",
       "      <th>numerical_statistics.max</th>\n",
       "      <th>numerical_statistics.approximate_num_distinct_values</th>\n",
       "      <th>numerical_statistics.completeness</th>\n",
       "      <th>numerical_statistics.distribution.kll.buckets</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.c</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.k</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LeaveOrNot</td>\n",
       "      <td>Integral</td>\n",
       "      <td>3722</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343632</td>\n",
       "      <td>1279.0</td>\n",
       "      <td>0.474920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Education</td>\n",
       "      <td>Integral</td>\n",
       "      <td>3722</td>\n",
       "      <td>0</td>\n",
       "      <td>0.265986</td>\n",
       "      <td>990.0</td>\n",
       "      <td>0.521096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.2, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JoiningYear</td>\n",
       "      <td>Integral</td>\n",
       "      <td>3722</td>\n",
       "      <td>0</td>\n",
       "      <td>2015.070661</td>\n",
       "      <td>7500093.0</td>\n",
       "      <td>1.865897</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 2012.0, 'upper_bound': 2012.6...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[2013.0, 2013.0, 2012.0, 2015.0, 2016.0, 2018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>City</td>\n",
       "      <td>Integral</td>\n",
       "      <td>3722</td>\n",
       "      <td>0</td>\n",
       "      <td>0.788823</td>\n",
       "      <td>2936.0</td>\n",
       "      <td>0.837401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.2, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PaymentTier</td>\n",
       "      <td>Integral</td>\n",
       "      <td>3722</td>\n",
       "      <td>0</td>\n",
       "      <td>2.694519</td>\n",
       "      <td>10029.0</td>\n",
       "      <td>0.566309</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 1.0, 'upper_bound': 1.2, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Age</td>\n",
       "      <td>Integral</td>\n",
       "      <td>3722</td>\n",
       "      <td>0</td>\n",
       "      <td>29.430682</td>\n",
       "      <td>109541.0</td>\n",
       "      <td>4.844176</td>\n",
       "      <td>22.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 22.0, 'upper_bound': 23.9, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[23.0, 25.0, 26.0, 39.0, 30.0, 24.0, 28.0, 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gender</td>\n",
       "      <td>Integral</td>\n",
       "      <td>3722</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600484</td>\n",
       "      <td>2235.0</td>\n",
       "      <td>0.489799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EverBenched</td>\n",
       "      <td>Integral</td>\n",
       "      <td>3722</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100215</td>\n",
       "      <td>373.0</td>\n",
       "      <td>0.300286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ExperienceInCurrentDomain</td>\n",
       "      <td>Integral</td>\n",
       "      <td>3722</td>\n",
       "      <td>0</td>\n",
       "      <td>2.904352</td>\n",
       "      <td>10810.0</td>\n",
       "      <td>1.560180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.7, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[1.0, 3.0, 4.0, 2.0, 3.0, 2.0, 1.0, 4.0, 1.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name inferred_type  \\\n",
       "0                 LeaveOrNot      Integral   \n",
       "1                  Education      Integral   \n",
       "2                JoiningYear      Integral   \n",
       "3                       City      Integral   \n",
       "4                PaymentTier      Integral   \n",
       "5                        Age      Integral   \n",
       "6                     Gender      Integral   \n",
       "7                EverBenched      Integral   \n",
       "8  ExperienceInCurrentDomain      Integral   \n",
       "\n",
       "   numerical_statistics.common.num_present  \\\n",
       "0                                     3722   \n",
       "1                                     3722   \n",
       "2                                     3722   \n",
       "3                                     3722   \n",
       "4                                     3722   \n",
       "5                                     3722   \n",
       "6                                     3722   \n",
       "7                                     3722   \n",
       "8                                     3722   \n",
       "\n",
       "   numerical_statistics.common.num_missing  numerical_statistics.mean  \\\n",
       "0                                        0                   0.343632   \n",
       "1                                        0                   0.265986   \n",
       "2                                        0                2015.070661   \n",
       "3                                        0                   0.788823   \n",
       "4                                        0                   2.694519   \n",
       "5                                        0                  29.430682   \n",
       "6                                        0                   0.600484   \n",
       "7                                        0                   0.100215   \n",
       "8                                        0                   2.904352   \n",
       "\n",
       "   numerical_statistics.sum  numerical_statistics.std_dev  \\\n",
       "0                    1279.0                      0.474920   \n",
       "1                     990.0                      0.521096   \n",
       "2                 7500093.0                      1.865897   \n",
       "3                    2936.0                      0.837401   \n",
       "4                   10029.0                      0.566309   \n",
       "5                  109541.0                      4.844176   \n",
       "6                    2235.0                      0.489799   \n",
       "7                     373.0                      0.300286   \n",
       "8                   10810.0                      1.560180   \n",
       "\n",
       "   numerical_statistics.min  numerical_statistics.max  \\\n",
       "0                       0.0                       1.0   \n",
       "1                       0.0                       2.0   \n",
       "2                    2012.0                    2018.0   \n",
       "3                       0.0                       2.0   \n",
       "4                       1.0                       3.0   \n",
       "5                      22.0                      41.0   \n",
       "6                       0.0                       1.0   \n",
       "7                       0.0                       1.0   \n",
       "8                       0.0                       7.0   \n",
       "\n",
       "   numerical_statistics.approximate_num_distinct_values  \\\n",
       "0                                                  2      \n",
       "1                                                  3      \n",
       "2                                                  7      \n",
       "3                                                  3      \n",
       "4                                                  3      \n",
       "5                                                 20      \n",
       "6                                                  2      \n",
       "7                                                  2      \n",
       "8                                                  8      \n",
       "\n",
       "   numerical_statistics.completeness  \\\n",
       "0                                1.0   \n",
       "1                                1.0   \n",
       "2                                1.0   \n",
       "3                                1.0   \n",
       "4                                1.0   \n",
       "5                                1.0   \n",
       "6                                1.0   \n",
       "7                                1.0   \n",
       "8                                1.0   \n",
       "\n",
       "       numerical_statistics.distribution.kll.buckets  \\\n",
       "0  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "1  [{'lower_bound': 0.0, 'upper_bound': 0.2, 'cou...   \n",
       "2  [{'lower_bound': 2012.0, 'upper_bound': 2012.6...   \n",
       "3  [{'lower_bound': 0.0, 'upper_bound': 0.2, 'cou...   \n",
       "4  [{'lower_bound': 1.0, 'upper_bound': 1.2, 'cou...   \n",
       "5  [{'lower_bound': 22.0, 'upper_bound': 23.9, 'c...   \n",
       "6  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "7  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "8  [{'lower_bound': 0.0, 'upper_bound': 0.7, 'cou...   \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.c  \\\n",
       "0                                               0.64           \n",
       "1                                               0.64           \n",
       "2                                               0.64           \n",
       "3                                               0.64           \n",
       "4                                               0.64           \n",
       "5                                               0.64           \n",
       "6                                               0.64           \n",
       "7                                               0.64           \n",
       "8                                               0.64           \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.k  \\\n",
       "0                                             2048.0           \n",
       "1                                             2048.0           \n",
       "2                                             2048.0           \n",
       "3                                             2048.0           \n",
       "4                                             2048.0           \n",
       "5                                             2048.0           \n",
       "6                                             2048.0           \n",
       "7                                             2048.0           \n",
       "8                                             2048.0           \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.data  \n",
       "0  [[0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0,...  \n",
       "1  [[2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...  \n",
       "2  [[2013.0, 2013.0, 2012.0, 2015.0, 2016.0, 2018...  \n",
       "3  [[1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0,...  \n",
       "4  [[3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0,...  \n",
       "5  [[23.0, 25.0, 26.0, 39.0, 30.0, 24.0, 28.0, 26...  \n",
       "6  [[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0,...  \n",
       "7  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "8  [[1.0, 3.0, 4.0, 2.0, 3.0, 2.0, 1.0, 4.0, 1.0,...  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the latest baselining job\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "\n",
    "# Normalize the JSON data\n",
    "schema_df = pd.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "\n",
    "# Display the first 10 rows of the DataFrame\n",
    "schema_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "94ae6278-346c-46f5-aadd-7099059a8fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>completeness</th>\n",
       "      <th>num_constraints.is_non_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LeaveOrNot</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Education</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JoiningYear</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>City</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PaymentTier</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Age</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gender</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EverBenched</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ExperienceInCurrentDomain</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name inferred_type  completeness  \\\n",
       "0                 LeaveOrNot      Integral           1.0   \n",
       "1                  Education      Integral           1.0   \n",
       "2                JoiningYear      Integral           1.0   \n",
       "3                       City      Integral           1.0   \n",
       "4                PaymentTier      Integral           1.0   \n",
       "5                        Age      Integral           1.0   \n",
       "6                     Gender      Integral           1.0   \n",
       "7                EverBenched      Integral           1.0   \n",
       "8  ExperienceInCurrentDomain      Integral           1.0   \n",
       "\n",
       "   num_constraints.is_non_negative  \n",
       "0                             True  \n",
       "1                             True  \n",
       "2                             True  \n",
       "3                             True  \n",
       "4                             True  \n",
       "5                             True  \n",
       "6                             True  \n",
       "7                             True  \n",
       "8                             True  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the JSON data for constraints\n",
    "constraints_df = pd.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "\n",
    "# Display the first 10 rows of the DataFrame\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee0f2a-eacb-4e56-bd8b-0ca0c154eb12",
   "metadata": {},
   "source": [
    "## Analyze data with Monitoring Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d6e9acb6-a01b-49ef-84f1-d23f0f0c51d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employee-monitor-endpoint\n",
      "employee-predictor\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "endpoints = sm_client.list_endpoints()\n",
    "\n",
    "for endpoint in endpoints['Endpoints']:\n",
    "    print(endpoint['EndpointName'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c77d536e-eff1-42a1-97e2-99f252e1687c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: employee-model-monitor-schedule\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator, EndpointInput, DefaultModelMonitor\n",
    "\n",
    "\n",
    "# Create the monitoring schedule\n",
    "schedule = my_default_monitor_1.create_monitoring_schedule(\n",
    "   monitor_schedule_name=schedule_name,\n",
    "   output_s3_uri=s3_report_path,\n",
    "   schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "   statistics=my_default_monitor.baseline_statistics(),\n",
    "   constraints=my_default_monitor.suggested_constraints(),\n",
    "   enable_cloudwatch_metrics=True,\n",
    "   endpoint_input=EndpointInput(\n",
    "        endpoint_name=endpoint_name,\n",
    "        destination=\"/opt/ml/processing/input/endpoint\",\n",
    "   )\n",
    ")\n",
    "\n",
    "# Print the schedule response\n",
    "print(schedule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a76f8-bf28-47af-8549-0ee9fc4b75a3",
   "metadata": {},
   "source": [
    "## Invoke the model with some generate sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3a61d569-1bc9-44f5-a863-a1bcbc3c7f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload: 2014,0,1,30,1,1,4, Response: 0.23545874655246735\n",
      "\n",
      "Payload: 2016,0,3,77,0,0,7, Response: 0.3164840042591095\n",
      "\n",
      "Payload: 2022,1,3,28,0,0,2, Response: 0.3164840042591095\n",
      "\n",
      "Payload: 2015,1,1,55,1,0,7, Response: 0.24510744214057922\n",
      "\n",
      "Payload: 2012,0,3,68,1,0,10, Response: 0.3164840042591095\n",
      "\n",
      "Payload: 2025,0,2,52,0,1,10, Response: 0.22797881066799164\n",
      "\n",
      "Payload: 2020,2,1,36,0,1,7, Response: 0.23545874655246735\n",
      "\n",
      "Payload: 2021,0,2,72,0,0,5, Response: 0.3164840042591095\n",
      "\n",
      "Payload: 2020,1,3,69,1,0,5, Response: 0.3164840042591095\n",
      "\n",
      "Payload: 2012,1,3,28,0,0,4, Response: 0.3164840042591095\n",
      "\n",
      "Payload: 2010,2,3,73,1,1,8, Response: 0.22797881066799164\n",
      "\n",
      "Payload: 2025,2,2,72,1,1,6, Response: 0.22797881066799164\n",
      "\n",
      "Payload: 2013,0,2,34,1,1,8, Response: 0.22797881066799164\n",
      "\n",
      "Payload: 2025,0,3,74,0,0,1, Response: 0.3164840042591095\n",
      "\n",
      "Payload: 2022,2,3,54,0,1,8, Response: 0.22797881066799164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "from time import sleep, time\n",
    "import boto3\n",
    "import random\n",
    "\n",
    "# Replace with your SageMaker session and the appropriate client\n",
    "sm_session = boto3.Session()\n",
    "runtime_client = sm_session.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Use the name of your deployed endpoint\n",
    "endpoint_name = 'employee-monitor-endpoint'\n",
    "\n",
    "\n",
    "# Define a function to generate sample data\n",
    "def generate_sample_data():\n",
    "    # Example: [JoiningYear, City, PaymentTier, Age, Gender, EverBenched, ExperienceInCurrentDomain]\n",
    "    sample = [\n",
    "        random.randint(2010, 2025),  # JoiningYear\n",
    "        random.randint(0, 2),        # City\n",
    "        random.randint(1, 3),        # PaymentTier\n",
    "        random.randint(20, 90),      # Age\n",
    "        random.randint(0, 1),        # Gender\n",
    "        random.randint(0, 1),        # EverBenched\n",
    "        random.randint(1, 10)        # ExperienceInCurrentDomain\n",
    "    ]\n",
    "    return ','.join(map(str, sample))\n",
    "\n",
    "def invoke_endpoint_with_generated_data(ep_name, runtime_client, max_invocations, max_duration):\n",
    "    start_time = time()\n",
    "    invocation_count = 0\n",
    "\n",
    "    while True:\n",
    "        # Check if the time limit has been reached\n",
    "        if time() - start_time > max_duration:\n",
    "            break\n",
    "\n",
    "        # Generate sample data and invoke the endpoint\n",
    "        payload = generate_sample_data()\n",
    "        response = runtime_client.invoke_endpoint(\n",
    "            EndpointName=ep_name,\n",
    "            ContentType=\"text/csv\",\n",
    "            Body=payload\n",
    "        )\n",
    "        print(f\"Payload: {payload}, Response: {response['Body'].read().decode('utf-8')}\")  # Print the response\n",
    "        \n",
    "        invocation_count += 1\n",
    "\n",
    "        # Check if the invocation limit has been reached\n",
    "        if invocation_count >= max_invocations:\n",
    "            break\n",
    "\n",
    "        sleep(1)  # Sleep for 1 second between requests\n",
    "\n",
    "def invoke_endpoint_limited():\n",
    "    max_invocations = 15  # Stop after 3 invocations\n",
    "    max_duration = 120  # Stop after 2 minutes (120 seconds)\n",
    "    invoke_endpoint_with_generated_data(endpoint_name, runtime_client, max_invocations, max_duration)\n",
    "\n",
    "# Start a thread to invoke the endpoint with generated data and given limits\n",
    "thread = Thread(target=invoke_endpoint_limited)\n",
    "thread.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194712d-6d73-41b2-afde-25a1392942f7",
   "metadata": {},
   "source": [
    "Should show \"Scheduled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6c49381f-eb2d-4c40-b674-e2599576845d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATUS: Scheduled\n"
     ]
    }
   ],
   "source": [
    "desc_schedule_result = my_default_monitor_1.describe_schedule()\n",
    "print(\"STATUS: {}\".format(desc_schedule_result[\"MonitoringScheduleStatus\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "eda206fc-0bdd-446e-86be-0aad83875cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<sagemaker.model_monitor.model_monitoring.MonitoringExecution at 0x7fd21d1b2d70>]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_default_monitor_1.list_executions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "095e9da1-ac34-404e-ad1f-858fd28a10d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've set up an hourly schedule that starts executions at the top of the hour (with a 0-20 minute buffer). We'll need to wait until the next hour for the first execution.\n",
      "We have 2 executions started.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a_my_executions = my_default_monitor_1.list_executions()\n",
    "print(\n",
    "    \"We've set up an hourly schedule that starts executions at the top of the hour (with a 0-20 minute buffer). We'll need to wait until the next hour for the first execution.\"\n",
    "\n",
    ")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    my_executions = my_default_monitor_1.list_executions()\n",
    "    print(\"Waiting for the first execution to start...\")\n",
    "    time.sleep(10)\n",
    "else:\n",
    "    my_executions = my_default_monitor_1.list_executions()\n",
    "    print(f\"We have {len(my_executions)} executions started.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a88e8ea1-5e03-420c-b580-d003af6acccb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest execution status: Completed\n",
      "Latest execution result: CompletedWithViolations: Job completed successfully with 1 violations.\n"
     ]
    }
   ],
   "source": [
    "latest_execution = my_executions[-1]  # Latest execution's index is -1\n",
    "\n",
    "\n",
    "print(\"Latest execution status: {}\".format(latest_execution.describe()[\"ProcessingJobStatus\"]))\n",
    "print(\"Latest execution result: {}\".format(latest_execution.describe()[\"ExitMessage\"]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a2b50737-216e-43a3-ba86-83a2b0c209a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATUS: Scheduled\n"
     ]
    }
   ],
   "source": [
    "desc_schedule_result = my_default_monitor_1.describe_schedule()\n",
    "print(\"STATUS: {}\".format(desc_schedule_result[\"MonitoringScheduleStatus\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ba90bda8-bede-49cb-847e-e386aaa824d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report Uri: s3://sagemaker-ml-28573/model-monitor/monitoring-results//employee-monitor-endpoint/employee-model-monitor-schedule/2024/09/04/14\n"
     ]
    }
   ],
   "source": [
    "report_uri = latest_execution.output.destination\n",
    "print(\"Report Uri: {}\".format(report_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "edcaf39f-d5fa-45b0-9385-b32c7abd5e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ml-28573/model-monitor/monitoring-results/\n"
     ]
    }
   ],
   "source": [
    "print(s3_report_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da35d297-b4e9-4671-95d4-bf3a72d2dba7",
   "metadata": {},
   "source": [
    "## View Generated Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f153c889-ff5b-4733-96b2-78e2ca0b8afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report bucket: sagemaker-ml-28573\n",
      "Report key: model-monitor/monitoring-results//employee-monitor-endpoint/employee-model-monitor-schedule/2024/09/04/14\n",
      "Found Report Files:\n",
      "model-monitor/monitoring-results//employee-monitor-endpoint/employee-model-monitor-schedule/2024/09/04/14/constraint_violations.json\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "s3uri = urlparse(report_uri)\n",
    "report_bucket = s3uri.netloc\n",
    "report_key = s3uri.path.lstrip(\"/\")\n",
    "print(\"Report bucket: {}\".format(report_bucket))\n",
    "print(\"Report key: {}\".format(report_key))\n",
    "\n",
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=report_bucket, Prefix=report_key)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Report Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9229b-bcba-4736-8777-6e626bef3f83",
   "metadata": {},
   "source": [
    "### Violations report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7db20b1c-7de3-4b5c-b6e6-f0114782d152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully.\n",
      "      feature_name constraint_check_type  \\\n",
      "0  Missing columns  missing_column_check   \n",
      "\n",
      "                                                                                                                           description  \n",
      "0  There are missing columns in current dataset. Number of columns in current dataset: 8, Number of columns in baseline constraints: 9  \n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define your bucket name and the key\n",
    "bucket_name = 'sagemaker-ml-28573'\n",
    "key = 'model-monitor/monitoring-results//employee-monitor-endpoint/employee-model-monitor-schedule/2024/09/04/14/constraint_violations.json'\n",
    "\n",
    "# Try to download the file\n",
    "try:\n",
    "    s3.download_file(bucket_name, key, 'constraint_violations.json')\n",
    "    print(\"File downloaded successfully.\")\n",
    "    \n",
    "    # Load and process the JSON file\n",
    "    with open('constraint_violations.json', 'r') as f:\n",
    "        violations = json.load(f)\n",
    "\n",
    "    constraints_df = pd.json_normalize(violations['violations'])\n",
    "    pd.set_option(\"display.max_colwidth\", None)\n",
    "    print(constraints_df.head(10))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "edb06ef0-d6a8-42f0-a7f7-d3022c8f275e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>constraint_check_type</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Missing columns</td>\n",
       "      <td>missing_column_check</td>\n",
       "      <td>There are missing columns in current dataset. Number of columns in current dataset: 8, Number of columns in baseline constraints: 9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_name constraint_check_type  \\\n",
       "0  Missing columns  missing_column_check   \n",
       "\n",
       "                                                                                                                           description  \n",
       "0  There are missing columns in current dataset. Number of columns in current dataset: 8, Number of columns in baseline constraints: 9  "
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open('constraint_violations.json', 'r') as f:\n",
    "    violations = json.load(f)\n",
    "    \n",
    "# Convert the 'violations' section of the JSON data into a DataFrame\n",
    "constraints_df = pd.json_normalize(violations['violations'])\n",
    "\n",
    "# Set Pandas options to display the full width of each column\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "constraints_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "27a19386-331c-4ee3-8700-6f922c868c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Stopping Monitoring Schedule with name: employee-model-monitor-schedule\n"
     ]
    }
   ],
   "source": [
    "my_default_monitor_1.stop_monitoring_schedule() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "54a54bf8-98b2-4d52-a7d4-f5811632b317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Stopping Monitoring Schedule with name: employee-model-monitor-schedule\n",
      "INFO:sagemaker:Deleting Monitoring Schedule with name: employee-model-monitor-schedule\n",
      "INFO:sagemaker.model_monitor.model_monitoring:Deleting Data Quality Job Definition with name: data-quality-job-definition-2024-09-04-13-37-47-080\n"
     ]
    }
   ],
   "source": [
    "my_default_monitor_1.stop_monitoring_schedule()\n",
    "my_default_monitor_1.delete_monitoring_schedule()\n",
    "time.sleep(60)  # Wait for the deletion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
